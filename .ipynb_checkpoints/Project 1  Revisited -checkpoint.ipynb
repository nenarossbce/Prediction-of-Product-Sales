{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b58b7f6",
   "metadata": {},
   "source": [
    "# Importances and Coefficients - Core\n",
    "\n",
    "Nena Esaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following command on your local computer to check the version of sklearn\n",
    "import sklearn\n",
    "!python --version\n",
    "print(f\"sklearn version: {sklearn.__version__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbbb4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "## Models & evaluation metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "## setting random state for reproducibility\n",
    "SEED = 321\n",
    "np.random.seed(SEED)\n",
    "## set pandas to display more columns\n",
    "pd.set_option('display.max_columns',50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b276aa9",
   "metadata": {},
   "source": [
    "### Load in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d13708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df = pd.read_csv(\"Data/sales_predictions_2023.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9345c6d9",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00456e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace inconsistent categories\n",
    "fat_content_map = {'LF':'Low Fat',\n",
    "                   'reg':'Regular',\n",
    "                   'low fat':'Low Fat'}\n",
    "df['Item_Fat_Content'] = df['Item_Fat_Content'].replace(fat_content_map)\n",
    "## Verify \n",
    "df['Item_Fat_Content'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop unwanted/inappropriate columns \n",
    "y = df['Item_Outlet_Sales'].copy()\n",
    "bad_cols = ['Outlet_Identifier','Outlet_Establishment_Year']\n",
    "X = X.drop(columns=bad_cols)\n",
    "## Perform a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b4551",
   "metadata": {},
   "source": [
    "#### Making a Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e9e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create categorical pipeline\n",
    "cat_selector = make_column_selector(dtype_include='object')\n",
    "# create pipeline for handling categorical data\n",
    "impute_most_freq = SimpleImputer(strategy='most_frequent')\n",
    "encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "cat_pipe = make_pipeline(impute_most_freq,encoder)\n",
    "## Create numeric pipelien\n",
    "num_selector = make_column_selector(dtype_include='number')\n",
    "num_selector(X_train)\n",
    "# create pipeline for handling categorical data\n",
    "impute_mean = SimpleImputer(strategy='mean')\n",
    "scaler = StandardScaler()\n",
    "num_pipe = make_pipeline(impute_mean, scaler)\n",
    "## Combine into 1 column transformer\n",
    "preprocessor = make_column_transformer( (cat_pipe,cat_selector),\n",
    "                                       (num_pipe,num_selector),\n",
    "                                      verbose_feature_names_out=False)\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit column transformer and run get_feature_names_out\n",
    "preprocessor.fit(X_train)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc146f",
   "metadata": {},
   "source": [
    "### Project 1 Revisited - Part 1: Remaking, Saving, and Explaining Your Models\n",
    "\n",
    "* Remake your X_train and X_test as DataFrames with the feature names extracted from the column transformer instead of combining your preprocessor and model into 1 pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21920519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train as dataframe \n",
    "X_train_df = pd.DataFrame(preprocessor.transform(X_train),\n",
    "                           columns = feature_names, index = X_train.index)\n",
    "X_train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80cf4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test as dataframe \n",
    "X_test_df = pd.DataFrame(preprocessor.transform(X_test),\n",
    "                           columns = feature_names, index = X_test.index)\n",
    "X_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6af250",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm the first 3 rows index in y_test matches X_test_df\n",
    "y_test.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ddf655",
   "metadata": {},
   "source": [
    "Custom Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5710d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(model, X_train,y_train, X_test, y_test): \n",
    "    \"\"\"Evaluates a scikit learn regression model using r-squared and RMSE\"\"\"\n",
    "    \n",
    "    ## Training Data\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    r2_train = metrics.r2_score(y_train, y_pred_train)\n",
    "    rmse_train = metrics.mean_squared_error(y_train, y_pred_train, \n",
    "                                            squared=False)\n",
    "    \n",
    "    print(f\"Training Data:\\tR^2= {r2_train:.2f}\\tRMSE= {rmse_train:.2f}\")\n",
    "        \n",
    "    \n",
    "    ## Test Data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    r2_test = metrics.r2_score(y_test, y_pred_test)\n",
    "    rmse_test = metrics.mean_squared_error(y_test, y_pred_test, \n",
    "                                            squared=False)\n",
    "    \n",
    "    print(f\"Test Data:\\tR^2= {r2_test:.2f}\\tRMSE= {rmse_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1055c4",
   "metadata": {},
   "source": [
    "### LinearRegression\n",
    "\n",
    "   * Fit and evaluate your LinearRegresion model using your dataframe X_train and X_test data.\n",
    "   \n",
    "   * Extract and visualize the coefficients that your model determined.\n",
    "   \n",
    "        * Select the top 3 most impactful features and interpret their coefficients in plain English.\n",
    "        \n",
    "   * Save your figure as a .png file inside your repository (you will need this for the final piece of this assignment - Update Your README)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29119f66",
   "metadata": {},
   "source": [
    "#### Fitting a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b8381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9066e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_df, y_train)\n",
    "evaluate_regression(lin_reg, X_train_df, y_train, X_test_df,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226d19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffac7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the coefficients\n",
    "coeffs = pd.Series(lin_reg.coef_, index= feature_names)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .loc to add the intercept to the series\n",
    "coeffs.loc['intercept'] = lin_reg.intercept_\n",
    "coeffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: f\"{x:,.2f}\")\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_hbars(ax, ha='left', va='center', size=12, xytext=(4,0),\n",
    "                  textcoords='offset points'):\n",
    "    for bar in ax.patches:\n",
    "    \n",
    "        ## calculate center of bar\n",
    "        bar_ax = bar.get_y() + bar.get_height() / 2\n",
    "        ## get the value to annotate\n",
    "        val = bar.get_width()\n",
    "        if val < 0:\n",
    "            val_pos = 0\n",
    "        else:\n",
    "            val_pos = val\n",
    "        # ha and va stand for the horizontal and vertical alignment\n",
    "        ax.annotate(f\"{val:.3f}\", (val_pos,bar_ax), ha=ha, va=va, size=size,\n",
    "                        xytext=xytext, textcoords=textcoords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5254160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coeffs(coeffs, top_n=None, figsize=(4,5), \n",
    "                intercept=False, intercept_name=\"intercept\", \n",
    "                annotate=False, ha='left', va='center', size=12, \n",
    "                xytext=(4,0), textcoords='offset points'):\n",
    "    \"\"\" Plots the top_n coefficients from a Series, with optional annotations.\n",
    "    \"\"\"\n",
    "    # Drop intercept if intercept=False and \n",
    "    if (intercept == False) & (intercept_name in coeffs.index):\n",
    "        coeffs = coeffs.drop(intercept_name)\n",
    "    if top_n == None:\n",
    "        ## sort all features and set title\n",
    "        plot_vals = coeffs.sort_values()\n",
    "        title = \"All Coefficients - Ranked by Magnitude\"\n",
    "    else:\n",
    "        ## rank the coeffs and select the top_n\n",
    "        coeff_rank = coeffs.abs().rank().sort_values(ascending=False)\n",
    "        top_n_features = coeff_rank.head(top_n)\n",
    "        \n",
    "        ## sort features and keep top_n and set title\n",
    "        plot_vals = coeffs.loc[top_n_features.index].sort_values()\n",
    "        title = f\"Top {top_n} Largest Coefficients\"\n",
    "    ## plotting top N importances\n",
    "    ax = plot_vals.plot(kind='barh', figsize=figsize)\n",
    "    ax.set(xlabel='Coefficient', \n",
    "            ylabel='Feature Names', \n",
    "            title=title)\n",
    "    ax.axvline(0, color='k')\n",
    "    if annotate == True:\n",
    "        annotate_hbars(ax, ha=ha, va=va, size=size, xytext=xytext, textcoords=textcoords)\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34648ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coeffs(coeffs, top_n=15, annotate=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c03a5",
   "metadata": {},
   "source": [
    "Select the top 3 most impactful features and interpret their coefficients in plain English\n",
    "\n",
    "* \n",
    "*\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41eae15",
   "metadata": {},
   "source": [
    "### Tree-Based Model\n",
    "\n",
    "   * Fit and evaluate your tree-based regression model using your dataframe X_train and X_test data.\n",
    "   \n",
    "   * Extract and visualize the feature importances that your model determined.\n",
    "   \n",
    "        * Identify the top 5 most important features.\n",
    "       \n",
    "   * Save your figure as a .png file inside your repository  (you will need this for the final piece of this assignment - Update Your README)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fddb059",
   "metadata": {},
   "source": [
    "#### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(random_state=SEED)\n",
    "reg.fit(X_train_df, y_train)\n",
    "evaluate_regression(reg, X_train_df, y_train, X_test_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b968cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract and Plot the Feature Importances\n",
    "reg.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3590568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the importances as a Pandas Series\n",
    "#saving the features importances \n",
    "importances = pd.Series(reg.feature_importances_, index=feature_names,\n",
    "                       name = 'Feature Importance')\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the features importances sorted from largest to smallest (ascending=False)\n",
    "sorted_importance = importances.sort_values(ascending=False)\n",
    "sorted_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a21b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## just keep the top 10 importances and plot (that are now at the bottom of our series)\n",
    "ax = sorted_importance.tail(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34659b",
   "metadata": {},
   "source": [
    "Identify the top 5 most important features\n",
    "\n",
    "*\n",
    "*\n",
    "*\n",
    "*\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94a647",
   "metadata": {},
   "source": [
    "### Serialize Your Best Models with Joblib\n",
    "   * Once you've finished updating and explaining your models, you must save the following key: value pairs as a dictionary in a joblib file named \"best-models.joblib\":\n",
    "   \n",
    "        * \"preprocessor\": your preprocessing  column transformer\n",
    "        \n",
    "        * \"X_train\": your training features.\n",
    "        \n",
    "        * \"X_test\": your test features.\n",
    "        \n",
    "        * \"y_train\": your training target.\n",
    "        \n",
    "        * \"y_test\": your test target.\n",
    "        \n",
    "        * \"LinearRegression\": your best linear regression\n",
    "        \n",
    "        * Your tree-based model's class name: your best tree-based model.\n",
    "            * e.g. \"RandomForestRegressor\"/\"DecisionTreeRegressor\"\n",
    "        \n",
    "        * Save your joblib file inside your repository. (You will work with these models again in the next core assignment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc9f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving variables for next lesson/notebook\n",
    "import joblib\n",
    "## creating a dictionary of all of the variables to save for later\n",
    "export = {'X_train':X_train_df,\n",
    "         'y_train': y_train,\n",
    "         'X_test':X_test_df,\n",
    "          \"y_test\": y_test,\n",
    "         'preprocessor':preprocessor,\n",
    "          'LinearRegression':lin_reg,\n",
    "         'RandomForest':reg}\n",
    "joblib.dump(export, 'best-models.joblib')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
